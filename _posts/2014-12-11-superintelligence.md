---
layout: post
title:  "Superintelligence"
authors: Walter Issacson
categories:
- books

---

Wanted to read this books as Elon Musk recommended it a few times. Possibly my first introduction to AI.


Why give a thought to AI?

> Quoth he: “This will surely be our undoing. Should we not give **some thought to the art of owl-domestication and owl-taming first**, before we bring such a creature into our midst?”

Fear of not understanding / taming AI

> They soon realized that Pastus had been right: this was an exceedingly difficult challenge, especially in the absence of an actual owl to practice on. Nevertheless they pressed on as best they could, constantly fearing that the flock might return with an owl egg **before a solution to the control problem had been found**.

What makes humans different from other animals

> Inside your cranium is the thing that does the reading. This thing, the human brain, has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that we owe our dominant position on the planet.

Compound knowledge

> The advantage has compounded over time, as each generation has built on the achievements of its predecessors.

One chance to get it right?

> It also looks like we will only get one chance. Once unfriendly superintelligence exists, it would prevent us from replacing it or changing its preferences. Our fate would be sealed.

Specialisation

> Especially after the adoption of agriculture, population densities rose along with the total size of the human population. **More people meant more ideas; greater densities meant that ideas could spread more readily** and that some individuals could devote themselves to developing specialized skills.

Doubling economy

> characteristic world economy doubling time for Pleistocene hunter–gatherer society of 224,000 years; for farming society, 909 years; and for industrial society, 6.3 years.

Early days of future technology

> **most technologies that will have a big impact on the world in five or ten years from now are already in limited use**, while technologies that will reshape the world in **less than fifteen years probably exist as laboratory prototypes**.

Last invention?

> Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the **first ultraintelligent machine is the last invention that man need ever make**, provided that the machine is docile enough to tell us how to keep it under control.

Invent logical proof

> came up with one proof that was much more elegant than the original, thereby debunking the notion that machines could “only think numerically” and showing that machines were also **able to do deduction and to invent logical proofs**.

Complex tasks

> In later decades, systems would be created that demonstrated that machines could **compose music** in the style of various classical composers, outperform junior doctors in certain **clinical diagnostic tasks**, drive cars autonomously, and make **patentable inventions**.

Smaller vs larger

> Hundreds of these expert systems were built. However, the smaller systems provided little benefit, and the larger ones proved expensive to develop, validate, and keep updated, and were generally cumbersome to use.

Neural networks

> neural networks could learn from experience, finding natural ways of generalizing from examples and finding hidden statistical patterns in their input. This made the nets good at pattern recognition and classification problems.

Evolution methods

> Evolution-based methods, such as **genetic algorithms and genetic programming**, constitute another approach whose emergence helped end the second AI winter.

Efficient way vs evolution

> Without an efficient way to encode candidate solutions (a genetic language that matches latent structure in the target domain), evolutionary search tends to meander endlessly in a vast search space or get stuck at a local optimum. Even if a good representational format is found, **evolution is computationally demanding** and is often defeated by the combinatorial explosion.

Bayesian networks

> Bayesian networks provide a concise way of **representing probabilistic and conditional independence** relations that hold in some particular domain.

New info

> As the agent receives **new information from its sensors, it updates its probability distribution by conditionalizing the distribution** on the new information according to Bayes’ theorem.

New view after AI

> In the view of several experts in the late fifties: “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.” This no longer seems so. One sympathizes with John McCarthy, who lamented: **“As soon as it works, no one calls it AI anymore.”**

Challenges

> **Common sense and natural language** understanding have also turned out to be difficult. It is now often thought that achieving a fully human-level performance on these tasks is an “AI-complete” problem, meaning that the difficulty of solving these problems is essentially equivalent to the difficulty of building generally human-level intelligent machines.

What is SUperintelligence?

> We can tentatively define a superintelligence as **any intellect that greatly exceeds the cognitive performance of humans** in virtually all domains of interest .

Learning is integral

> It now seems clear that **a capacity to learn would be an integral feature** of the core design of a system intended to attain general intelligence, not something to be tacked on later as an extension or an afterthought.

Child machine

> Turing envisaged **an iterative process to develop such a child machine**: We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns.

Evolution with greater efficiency

> We know that **blind evolutionary processes can produce human-level general intelligence**, since they have already done so at least once. Evolutionary processes with foresight—that is, genetic programs designed and guided by an intelligent human programmer—should be able to achieve a similar outcome with far greater efficiency.

Why should AI be possible?

> The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same.

Failure to match evolution

> However, one could equally point to areas where human engineers have thus far failed to match evolution: in morphogenesis, self-repair, and the immune defense, for example, human efforts lag far behind what nature has accomplished.

Costs of intelligence

> Even environments in which organisms with superior information processing skills reap various rewards may not select for intelligence, because improvements to intelligence can (and often do) impose significant costs, such as **higher energy consumption or slower maturation times**, and those costs may outweigh whatever benefits are gained from smarter behavior.

Hybrid

> Before this happens, though, it is possible that a hybrid approach, combining some **brain-inspired techniques with some purely artificial methods**, would cross the finishing line. In that case, the resultant system need not be recognizably brain-like even though some brain-derived insights were used in its development.

Different from nature

> Different people working toward machine intelligence hold different views about how promising neuromorphic approaches are compared with approaches that aim for completely synthetic designs. The **existence of birds demonstrated that heavier-than-air flight was physically possible and prompted efforts to build flying machines. Yet the first functioning airplanes did not flap their wings.**

Recursive improvements

> This brings us to another important concept, that of “recursive self-improvement.” **A successful seed AI would be able to iteratively enhance itself**: an early version of the AI could design an improved version of itself, and the improved version—being smarter than the original—might be able to design an even smarter version of itself, and so forth.

Digital reproduction

> If completely successful, the result would be a digital reproduction of the original intellect, with memory and personality intact. The emulated human mind now exists as software on a computer. The mind can either inhabit a virtual reality or interface with the external world by means of robotic appendages.

Whole brain emulation

> Whole brain emulation does, however, require some rather advanced enabling technologies. There are three key prerequisites: (1) scanning: high-throughput microscopy with sufficient resolution and detection of relevant properties; (2) translation: automated image analysis to turn raw scanning data into an interpreted three-dimensional model of relevant neurocomputational elements; and (3) simulation: hardware powerful enough to implement the resultant computational structure

Stupidest possible

> Far from being the smartest possible biological species, we are probably better thought of as the **stupidest possible biological species capable of starting a technological civilization** —a niche we filled because we got there first, not because we are in any sense optimally adapted to it.

Collective intelligence

> In general terms, a **system’s collective intelligence is limited by the abilities of its member minds**, the overheads in communicating relevant information between them, and the various distortions and inefficiencies that pervade human organizations.

Growth of collective intelligence

> Growth in collective intelligence may also come from **more general organizational and economic improvements**, and from enlarging the fraction of the world’s population that is educated, digitally connected, and integrated into global intellectual culture. 82

3 types of superintelligence

> three forms: **speed** superintelligence, **collective** superintelligence, and **quality** superintelligence.

Collective intelligence !== Wiser

> not the case that contemporary humanity is idolizing material consumption, depleting natural resources, polluting the environment, decimating species diversity, all the while failing to remedy screaming global injustices and neglecting paramount humanistic or spiritual values? However, setting aside the question of how modernity’s shortcomings stack up against the not-so-inconsiderable failings of earlier epochs, **nothing in our definition of collective superintelligence implies that a society with greater collective intelligence is necessarily better off**. The definition does not even imply that the more collectively intelligent society is wiser. We can think of **wisdom as the ability to get the important things approximately right**.

Hypothesis

> Suppose that a scientific genius of the caliber of a **Newton or an Einstein arises at least once for every 10 billion people**: then on MegaEarth there would be 700,000 such geniuses living contemporaneously, alongside proportionally vast multitudes of slightly lesser talents. **New ideas and technologies would be developed at a furious pace**, and global civilization on MegaEarth would constitute a loosely integrated collective superintelligence.

Implementational inefficiencies

> Sooner or later, the most glaring implementational inefficiencies will have been optimized away, the most promising algorithmic variations will have been tested, and the easiest opportunities for organizational innovation will have been exploited.

Increase in knowledge diffusion

> Over long historical timescales, **there has been an increase in the rate at which knowledge and technology diffuse around the globe**. As a result, the temporal gaps between technology leaders and nearest followers have narrowed.

Global ages accelerated with each age

> 8 On a grander scale, the human species took tens of thousands of years to spread across most of the globe, the Agricultural Revolution thousands of years, the Industrial Revolution only hundreds of years, and an Information Revolution could be said to have spread globally over the course of decades—though, of course, **these transitions are not necessarily of equal profundity**.

How short can a period be?

> It is possible that globalization and increased surveillance will reduce typical lags between competing technology projects. Yet **there is likely to be a lower bound on how short the average lag could become** (in the absence of deliberate coordination). Even absent dynamics that lead to snowball effects, some projects will happen to end up with better research staff, leadership, and infrastructure, or will just stumble upon better ideas.

Can mega projects on AI be possible?

> There are precedents of **large-scale successful multinational scientific collaborations**, such as the International Space Station, the Human Genome Project, and the Large Hadron Collider. However, the **major motivation for collaboration in those cases was cost-sharing**. (In the case of the International Space Station, fostering a collaborative spirit between Russia and the United States was itself an important goal.) Achieving similar collaboration on a project that has enormous security implications would be more difficult.

Careful replication

> The superintelligent agent could design the von Neumann probes to be evolution-proof. This could be accomplished by careful quality control during the replication step. For example, the control software for a daughter probe could be proofread multiple times before execution, and the software itself could use encryption and error-correcting code to make it arbitrarily unlikely that any random mutation would be passed on to its descendants.

Chance of survival with tools

> Twenty thousand years ago, say, with equipment no fancier than stone axes, bone tools, atlatls, and fire, the human species was perhaps already in a position from which it had an excellent chance of surviving to the present era.

Intelligence !== Motivation

> What is the **relation between intelligence and motivation in an artificial agent**? Here we develop two theses. The orthogonality thesis holds (with some caveats) that **intelligence and final goals are independent variables**: any level of intelligence could be combined with any final goal.

Intelligence

> By “intelligence” we here mean something like **skill at prediction, planning, and means–ends reasoning in general**. This sense of instrumental cognitive efficaciousness is most relevant when we are seeking to understand what the causal impact of a machine superintelligence might be.

Value on survival

> This creates an instrumental reason for the agent to try to be around in the future—to help achieve its future-oriented goal. Most humans **seem to place some final value on their own survival**. This is not a necessary feature of artificial agents: some may be designed to place no final value whatever on their own survival.

Goal vs Survival

> Goal-content integrity for final goals is in a sense even more fundamental than survival as a convergent instrumental motivation. Among humans, the opposite may seem to hold, but that is because survival is usually part of our final goals.

Where are the resources allocated for humans?

> A great deal of resource accumulation is motivated by **social concerns gaining status, mates, friends, and influence**, through wealth accumulation and conspicuous consumption. Perhaps less commonly, some people seek additional resources to **achieve altruistic ambitions or expensive non-social aims**.

Smarter AI == Safer

> So development continues, and progress is made. As the automated navigation systems of cars become smarter, they suffer fewer accidents; and as military robots achieve more precise targeting, they cause less collateral damage. A broad lesson is inferred from these observations of real-world outcomes: the smarter the AI, the safer it is. It is a lesson **based on science, data, and statistics, not armchair philosophizing**.

Treacherous turn?

> We observe here how it could be the case that when dumb, smarter is safer; yet when smart, smarter is more dangerous. There is a kind of pivot point, at which a strategy that has previously worked excellently suddenly starts to backfire. We may call the phenomenon the treacherous turn.

3 Laws of Robotics by Isaac Asimov

> The traditional illustration of the direct rule-based approach is the “three laws of robotics” concept, formulated by science fiction author Isaac Asimov in a short story published in 1942. The three laws were: (1) A robot **may not injure a human being** or, through inaction, allow a human being to come to harm; (2) A robot **must obey any orders given to it by human beings**, except where such orders would conflict with the First Law; (3) A robot must **protect its own existence** as long as such protection does not conflict with the First or Second Law.

Oracle == Q&A

> An oracle is a question-answering system. It might accept questions in a natural language and present its answers as text. An oracle that accepts only yes/no questions could output its best guess with a single bit, or perhaps with a few extra bits to represent its degree of confidence.

Genie == CLI

> A genie is a command-executing system: it receives a high-level command, carries it out, then pauses to await the next command.

Sovereign

> A sovereign is a system that has an open-ended mandate to operate in the world in pursuit of broad and possibly very long-range objectives.

Fair work practices

> One parameter that might be **relevant to consumer choice is the inner life of the worker** providing a service or product. A concert audience, for instance, might like to know that the performer is **consciously experiencing** the music and the venue.

Taking a long-term view

> Yet there are reasons, if we take a longer view and assume a state of unchanging technology and continued prosperity, to expect a return to the historically and ecologically normal condition of a world population that butts up against the limits of what our niche can support. If this seems counterintuitive in light of the negative relationship between wealth and fertility that we are currently observing on the global scale, we must remind ourselves that this modern age is a brief slice of history and very much an aberration.

Mental dispositions of past and present

> the mental dispositions that were adaptive for hunter–gatherer hominids roaming the African savanna **may not necessarily be adaptive for modified emulations living in post-transition** virtual realities.

Similarities between humans and the animals

> Many of the behaviors in question are not even unique to Homo sapiens. **Flamboyant display is found in a wide variety of contexts**, from sexual selection in the animal kingdom to **prestige contests among nation states**.

Documentary evidence > Costly displays

> Second, technologically advanced agents might have available new means of reliably communicating information about themselves, means that do not rely on costly display. Even today, when professional lenders assess creditworthiness they tend to **rely more on documentary evidence, such as ownership certificates and bank statements, than on costly displays, such as designer suits and Rolex watches**.

Abstraction layers

> This intuitive understanding of vision is like a duke’s understanding of his patriarchal household: as far as he is concerned, things simply appear at their appropriate times and places, **while the mechanism that produces those manifestations are hidden from view**.

Building ethics with Evolution

> Nature might be **a great experimentalist, but one who would never pass muster with an ethics review board** contravening the Helsinki Declaration and every norm of moral decency, left, right, and center. It is important that we not gratuitously replicate such horrors in silico. **Mind crime seems especially difficult to avoid when evolutionary methods are used to produce human-like intelligence**

Final values come from experiences

> Much of the information content in our final values is thus acquired from our experiences rather than preloaded in our genomes.

What is Friendliness?

> While the programmers are trying to explain to the seed AI what friendliness is, they might make errors in their explanations. Moreover, the programmers themselves may not fully understand the true nature of friendliness. One would therefore want the AI to have the ability to correct errors in the programmers’ thinking, and to infer the true or intended meaning from whatever imperfect explanations the programmers manage to provide.

Morality over time

> It is also reflected in the marked **changes that the distribution of moral belief has undergone over time**, many of which we like to think of as progress. In medieval Europe, for instance, it was deemed respectable entertainment to watch a political prisoner being tortured to death. Cat-burning remained popular in sixteenth-century Paris.

How to decide morality for the future?

> Very likely, we are still laboring under one or more grave moral misconceptions. In such circumstances to **select a final value based on our current convictions**, in a way that locks it in forever and precludes any possibility of further ethical progress, would be to **risk an existential moral calamity**.

Hijacking AI

> “Avoid hijacking the destiny of humankind” Yudkowsky has in mind a scenario in which a small group of programmers creates a seed AI that then grows into a superintelligence that obtains a decisive strategic advantage. In this scenario, the original programmers hold in their hands the entirety of humanity’s cosmic endowment.

Sounds like...God?

> “Keep humankind ultimately in charge of its own destiny” We might not want an outcome in which a **paternalistic superintelligence watches over us constantly, micromanaging our affairs with an eye towards optimizing every detail in accordance with a grand plan**. Even if we stipulate that the superintelligence would be perfectly benevolent, and free from presumptuousness, arrogance, overbearingness, narrow-mindedness, and other human shortcomings, **one might still resent the loss of autonomy entailed by such an arrangement**.

Creating our destiny

> We might **prefer to create our destiny as we go along, even if it means that we sometimes fumble**. Perhaps we want the superintelligence to **serve as a safety net**, to support us when things go catastrophically wrong, but otherwise to leave us to fend for ourselves.

Later the better?

> There are several quite strong reasons to believe that the riskiness of an intelligence explosion will decline significantly over a multidecadal timeframe. One reason is that a later date leaves more time for the development of solutions to the control problem.
